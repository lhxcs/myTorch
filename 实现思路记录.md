# 实现思路记录

## 前向反向

用一个全局的Graph去按照forward的顺序存储所有**算子**

算子(Op类)

grad_func:记录自己的导数

self.last: list     记录所有输入

bp时调用Graph中的倒序去挨个bp

即对每个算子的所有last去进行调用grad_func

至于上下游梯度的传递，我们在Tensor中添加一个father_Op

表示是哪个Op实例产生的这个Tensor

## 线性层(nn)

先实现矩阵乘法

用weight矩阵和bias矩阵组装一下实现forward

### broadcast处理

把广播理解成全连接层那种把一个变量同时给多个算子，bp时需把broadcast部分的导数累计求和

![a23302f2062b4298a44ff00275feae7](https://typorasyt.oss-cn-nanjing.aliyuncs.com/202408191025655.jpg)

(上图为邢存远学长回复)

到具体代码，forward中出现的broadcast不作处理。

bp中检测到，就按照broadcast的逆流程进行压缩

```
if a.ndim!=b.dim  #make sure a.ndim>b.ndim
	把b.dim中大小为1的维度找到，并在a中的对应维度进行压缩(np.sum,keepdim=True)
	把a.dim前面多出的那一段压缩掉(理论上都是维度大小都是1，用sum还是mean不太重要)
```

